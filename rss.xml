<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/css" href="rss.css" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
<title>Simon's Blog</title>
<description>The ramblings of a mathematician. LaTeX equations will probably turn into jumbo without MathJax, so I suggest you view my posts online. But if you must, here is an RSS feed.</description> <language>en-us</language>
<link>http://simonxiang.xyz/rss.xml</link>
<atom:link href="http://simonxiang.xyz/rss.xml" rel="self" type="application/rss+xml" />

<!-- LB -->

<item>
<title>Destroying my Computer or: How I Learned to Stop Worrying and Love Linux</title>
<guid>https://simonxiang.xyz/blog/destroying-my-computer-or-how-i-learned-to-stop-worrying-and-love-linux</guid>
<pubDate>Tue, 25 Aug 2020 07:50:17 -0500</pubDate>
<description><![CDATA[
<p>
So for the last two weeks I haven't really been online, due to the fact that I messed up my computer beyond repair. For some reason it brings a strange sense of comfort that <em>I</em> was the one that bricked it, as opposed to Microsoft or something. Yay for user freedoms!
</p>

<p>
This is what happened— Google was being annoying as usual and sent me my photos with each day having its own directory or something like that, preventing me from using my file manager to quickly live preview the photos. So a lightbulb went off in my head: why not write a script to move every file out of the directories and into the main one! It went something like this:
</p>

<pre>
<code>
for d a directory; do 
    cd /. && mv *.* ../ 
done
</code>
</pre>

<p>
or something dumb like that (not sure if this was the actual syntax). <em>Please</em> don't run this on your home computer.
</p>

<p>
So it ended up moving <b>every</b> file out of its original directory up one level: I didn't realize this at first and ran it again with sudo to get rid of the error messages (big mistake!). As soon as I did this I lost control of my keyboard and it hit me that I was kind of screwed.
</p>

<p>
The <code>/boot</code> directory was gone, making it extremely difficult for me to go in with <code>grub</code> and rescue at least some files. It makes sense, because everything in a UNIX system is a file. Some of the error messages I got as I attempted to save my system were truly horrific: "<code>Kernel panic - not syncing: Attemped to kill initf</code>", "<code>Trying to continue (this will most likely fail) ... </code>", and my personal favorite, 
</p>

<pre>
<code>
ERROR: Failed to mount the root device.
Bailing out, you are on your own. Good luck.
</code>
</pre>

<p>
Absolutely terrifying.
</p>

<p>
I've spent the last two weeks or so offline. After spending the last two months actively installing my system, customizing my dotfiles, getting programs to work with each other, hosting software on my server, writing blog posts, etc, I decided to take a break.
</p>

<p>
At first I was kind of stressed out: "What about my dotfiles? My LaTeX notes? How will I keep my streak of green on GitHub? What am I going to do in my free time?" It was like quitting social media all over again. It's the fear of missing out: the fear that something online is happening and you're not part of it. It's also the fear of boredom: in a generation of cell phones and constant stimulation, there's no one thing that collectively scares us more than being bored.
</p>
<p>
In the end, I learned to stop worrying. No matter what your brain says, you really aren't missing out on anything: rather, spending more time online means missing out on more of life. I had a lot of fun doing things like working on my bike, meeting up with friends at parks, learning to write again with the proper grip. 
</p>

<p>
On boredom: Boredom is the driving force that spurs action, without boredom there is no progress. Solitude and boredom are concepts essential to our mental development that have been washed away by the passage of the Internet. As time went on, I slowly began to dread the day I would reinstall Arch and rejoin the virtual world.
</p>

<p>
As you can see, I've managed to reinstall my system and get access to my server again. It took about a week to get everything working, but this time I documented all the commands I ran to fix all the bugs in a text file, so if I ever were to do this again, it would be as painless as 1-2-3 (the file is 230 lines long). Well, that's about it for now— I've also moved into campus, so if anybody reading this is from UT Austin, let's get in touch. Cheers!
</p>
]]></description>
</item>




<item>
<title>Math is Hard Because Our Short Term Memory Sucks</title>
<guid>https://simonxiang.xyz/blog/math-is-hard-because-our-short-term-memory-sucks</guid>
<pubDate>Fri, 07 Aug 2020 19:20:04 -0500</pubDate>
<description><![CDATA[
<blockquote>
<p>
I just came back from attending the 1052nd AMS (sectional) meeting at Penn State, last weekend, and realized that the Kingdom of Mathematics is dead. Instead we have a <b>disjoint</b> union of <em>narrow specialties</em>, and people who know everything about nothing, and nothing about anything (except their very narrow acre). Not only do they know nothing besides their narrow expertise, they don't care!
</p>
<cite>–Doron Zeilberger, Oct. 28, 2009</cite>
</blockquote>

<p>
Here's a hot take: all the ideas we describe in math are relatively easy to grasp, but mathematicians make things complicated because of our poor short term memory. To give an example, let's take Calculus— the ideas behind derivatives/integration are really simple (limit of sums, limit of slopes) but we make them difficult by introducing levels of rigor and notation that leave a first year Calculus student behind in the dust. 
</p>

<p>
This applies to higher level concepts as well: I believe that groups, fields, metric spaces, topological spaces, etc are fundamentally simple, but they suddenly aren't because of the level of rigor needed to formally approach these topics.
</p>

<p>
Rigor arises as a process to justify logical reason, but if our long term memory capacity was higher and we remembered much more of concepts learnt earlier, then there would be no need to obfuscate math with weird symbols and definitions. We add the rigor to remind ourselves that our reasoning related to past subjects is correct, which leads to rigor being required to have further areas of study utilize the current topic. 
</p>

<p>
Of course, without rigor there would be no certainty, which is pretty much the point of math itself. So I guess we wouldn't be studying math, it would just be another branch of science, or maybe even just pseudoscience. On the flip side, solving this issue could possibly lead to the Pre-20th century level of unification in mathematics, one that hasn't been seen since David Hilbert.
</p>

<p>
Ever since the 20th century, mathematics has become so vast and fractured that specialization is the only way to survive in the field. No one can claim to truly understand more than a couple of hyper-specialized fields anymore due to the immense time and effort it takes to read enough papers to become knowledgable in a field. To quote Doron Zeilberger once again, we no longer have "<em>mathematicians</em>", but instead we have "<em>topological algebraic Lie theorists, algebraic analytic number theorists, pseudo-spectral graph theorists</em> etc".
</p>

<p>
This is, of course, not the fault of our mathematicians, but a natural consequence of the direction the field has been going, (and I argue) as well as our natural short term memory capacity. In an ideal world where our brains were constructed differently, we would have mathematicians consulting and working with a plethora of other mathematicians of topics at the highest level, research and developments realidly accessible to the layman that puts in 2 weeks as opposed to 20 years to understand the work, and a rapid flow of major breakthroughs (after all, great proofs like the proof of Fermat's Last Theorem utilized the hyper-specialized work of many many fields).
</p>

<p>
But of course, this is just wishful thinking and entertaining a fantasy "what-if" situation. Or maybe it's just BS, and if we truly could remember more, mathematics would still end up the way it is to preserve the aspect of exclusivisity and prestige. However, it's something interesting to think about: if we were given a choice between psuedoscience and progress as opposed to rigor and fragmentation, which would mathematicians pick?
</p>
<p style="font-size: 14px">
References: <a href="https://sites.math.rutgers.edu/~zeilberg/Opinion104.html
" target="_blank">https://sites.math.rutgers.edu/~zeilberg/Opinion104.html
</a>
</p>
]]></description>
</item>




<item>
<title>Linear Algebra and Its Applications, Part 2: Derivatives</title>
<guid>https://simonxiang.xyz/blog/linear-algebra-and-its-applications-part-2-derivatives</guid>
<pubDate>Wed, 05 Aug 2020 21:50:57 -0500</pubDate>
<description><![CDATA[
<p>
I mentioned in my introduction post that this series would probably end up being about the applications of Linear Algebra to other fields of math or something. Well it's the first post, and we already stopped talking about real life applications! Whoops. 
</p>
<p>
Consider the vector space with basis
\[
\mathscr{B} = \{ 1, x, x^2, ... , x^n \}
\]
over $\mathbb{R}.$ This is known as the <b>vector space of polynomials with real coefficients of degree n or less</b>, denoted by $\mathbb{R}[x]$ (some texts may use $P_n)$. If it looks familiar, it probably showed up as a frequent example in your Linear Algebra problem sets (and is of extreme importance over an arbitrary field in Galois Theory)! 
</p>
<p>Note: while in Algebra the PID $\mathbb{F}[x]$ for $\mathbb{F}$ a field contains infinite degree polynomials, in this case we will assume $\mathbb{R}[x]$ to have finite degree polynomials with maximum degree $n$.
</p>

<p>
We can write an arbitrary element of any vector space as a <b>linear combination</b> of the elements of its basis set. In this case, an element of $\mathbb{R}[x]$ looks like a polynomial. A linear combination of the elements of $\mathscr{B}$ is of the form 
\[
a + a_1x + a_2x^2 + ... a_nx^n.
\]
We can use $f(x)$ and $g(x)$ to denote such linear combinations with shorthand $\sum_{i=0}^{n} a_ix^i$ for $a_i \in \mathbb{R}$. Note that  $x$ <em>isn't actually a variable:</em> we haven't defined a way to evaluate $x$ and it doesn't change based off the input. (If you're wondering how we evaluate polynomials in the traditional sense, we use something called the <em>evaluation homomorphism</em>).
</p>

<p>
We can use this vector space to do some cool things, like take ideas from Calculus and express them in the language of Linear Algebra! Remember the <b>derivative</b> from Calculus: in this case it's a map 
$\frac{d}{dx} : \mathbb{R}[x] \to \mathbb{R}[x]$ such that 
\[\frac{d}{dx}\left(a+a_1x+...+a_nx^n\right)=a_1+2a_2x+...+na_nx^{n-1}.\]
In summation notation, it's saying that 
\[\frac{d}{dx}\sum_{i=0}^{n} a_ix^i = \sum_{i=0}^{n-1} (i+1)a_{i+1}x^{i}.\]
Using the standard notations for derivatives, we can write $\frac{d}{dx}f(x)=f'(x)$ for $f(x)\in\mathbb{R}[x].$
</p>


<p>
We can show that the map $\frac{d}{dx} : \mathbb{R}[x] \to \mathbb{R}[x]$ is <b>linear</b>. Recall that the conditions for a map $T: V \to V$ to be linear are that 
\[
\text{1:}\,\,T(v_1+v_2)=T(v_1)+T(v_2)
\]
\[
\text{2:}\,\,T(\alpha v) = \alpha T(v)
\]
for a vector space $V$ over a field $\mathbb{F}, v_i \in V, \alpha \in \mathbb{F}.$ We know that 
\[
\frac{d}{dx}\left(f(x) + g(x)\right) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x)
\]
for $f(x), g(x) \in \mathbb{R}[x],$ satisfying the first condition. Next, 
\[
\frac{d}{dx}\left( c f(x) \right) = c \frac{d}{dx}f(x)
\]
for $c \in \mathbb{R}, f(x) \in \mathbb{R}[x],$ so $\frac{d}{dx}$ is linear.
</p>
<p>
Furthermore, if a map from a vector space onto itself is linear, we can say it's a <em><b>linear transformation.</b></em> We can represent such transformations with a <b>matrix.</b> To find the matrix representation of a linear transformation, examine the column vectors that arise from the <b>image of the basis set under such linear transformation.</b> We denote this matrix as $[T]_B$ for a vector space  $T$ with basis set $B$, so for the derivative operator we would denote the image of $\mathscr{B}$ under $\frac{d}{dx}$ as $[\frac{d}{dx}]_{\mathscr{B}}$.
</p>
<p>
To find the first column vector, examine the image of $1$ under $\frac{d}{dx}$: clearly it vanishes since constants don't change. So 
$
[1]_{\frac{d}{dx}}=
\Bigg[\begin{smallmatrix}
   0 \\
   \vdots \\
   0 \\
\end{smallmatrix}\Bigg]
$
Similarly, we have
\[ [x]_{\frac{d}{dx}} = 
\begin{bmatrix}
   1 \\
   0 \\
   \vdots \\
   0 \\
\end{bmatrix}, \,[x^2]_{\frac{d}{dx}} =
\begin{bmatrix}
   0 \\
   2 \\
   0 \\
   \vdots \\
   0 \\
   \end{bmatrix}, \,[x^3]_{\frac{d}{dx}} =
\begin{bmatrix}
   0 \\
   0 \\
   3 \\ 
   0 \\
   \vdots \\
   0 \\
   \end{bmatrix},
\]
and so on. Intuitively, this is because $\frac{d}{dx}x=1, \frac{d}{dx}x^2 = 2x, \frac{d}{dx}x^3=3x^2,$ etc, and we represent those values with the vectors above if we write them as a linear transformation of the elements of $\mathscr{B}$ (e.g. $2x=0\cdot 1+2x+0x^2 + \cdots$, $3x^2=0\cdot 1+0x+3x^2+0x^3+ \cdots).$
</p>
<p>
If we combine all these column vectors, we can find a matrix representation of the derivative! Here it is in all its glory:

\[
[\frac{d}{dx}]_{\mathscr{B}} = 
\begin{pmatrix}
   0 & 1 & 0 & 0 & \cdots & 0 \\
   0 & 0 & 2 & 0 & \cdots & 0 \\
   0 & 0 & 0 & 3 & \cdots & 0 \\ 
   0 & 0 & 0 & 0 & \ddots & \vdots \\
   \vdots & \vdots & \vdots & \vdots & \ddots & n \\
   0 & 0 & 0 & 0 & 0 & 0 \\ 
   \end{pmatrix},
\]
where $\dim (\mathbb{R}[x]) = n+1$ (or $\mathbb{R}[x]$ having polynomials of a maximum degree $n$). You can multiply this matrix by some polynomial vectors in your free time to see if this really works. 
</p>
<p>
Furthermore, the derivative matrix has an interesting algebraic property in that it's <b>nilpotent.</b> A nilpotent matrix is defined as one that eventually vanishes when multiplied by itself, that is, there exists some $k \in \mathbb{Z}$ such that 
\[
A^k = 0,
\]
where $A$ is a matrix and $0$ denotes the zero matrix. 
</p>
<p>
I won't offer a proof, but this is an intuitive result: recall from Analysis that no polynomial of finite degree is infinitely differentiable. So an arbitrary polynomial of degree $n$ will vanish if differentiated $n+1$ times, or in other words, multiplied by the matrix $\frac{d}{dx}^{n+1}$ (in Liebniz notation this would be denoted as $\frac{d^{n+1}}{dx^{n+1}}$). So clearly $\frac{d^{n+1}}{dx^{n+1}}$ corresponds to the zero matrix, and $k=n+1.$
</p>
]]></description>
</item>


<item>
<title>Algebra vs. Analysis: How do you eat your corn on the cob?</title>
<guid>https://simonxiang.xyz/blog/algebra-vs-analysis-how-do-you-eat-your-corn-on-the-cob</guid>
<pubDate>Mon, 03 Aug 2020 16:27:31 -0500</pubDate>
<description><![CDATA[
<p>
There's an interesting discussion about relating your mathematical field of study to the way you eat corn on the cob. This sounds ridiculous, but if you look closer, it's actually very interesting. Here's the hypothesis: the algebraists will eat their corn in rows, whereas analysts go in spirals.
</p>

<p>
Of course, this isn't a theorem because we have no way to prove this. It's also non-trivial to partition the set of mathematicians by the "algebra" or "analysis" equivalence classes (what of the mathematician that studies algebraic topology?). However, for some reason, this rule seems to hold in most cases. Regarding the mathematicians I talked to in real life, the one who wrote his PhD on Graph Theory eats his corn in rows, whereas the one who does research in Measure Theory eats his corn in spirals.
</p>

<p>
Personally, I liked my Algebra courses much more than my Analysis ones— I found them much more enlightening, intuitive, and interesting. People rave about the beauty of Analysis proofs, but I just saw them as confusing (perhaps this is due to the fact that Real Analysis was my first proof based course). The strange "theorem" about corn on the cob didn't really click with me until it became personal: after some experimentation, it turns out I eat my corn in rows.
</p>

<p>
 There isn't going to be a definitive answer as to why this happens, but we can guess. Here's my proposition: Algebra is all about analyzing structure, which is why algebraists will see the perfectly laid out rows and follow them. Analysis is about finding patterns, which is why analysts will seek out the spiral patterns and follow those instead.
</p>

<p style="font-size: 14px">
References: <a href="https://bentilly.blogspot.com/2010/08/analysis-vs-algebra-predicts-eating.html" target="_blank">https://bentilly.blogspot.com/2010/08/analysis-vs-algebra-predicts-eating.html</a>
</p>
]]></description>
</item>






















































<item>
<title>Want to Subscribe? Use RSS!</title>
<guid>https://simonxiang.xyz/blog/want-to-subscribe-use-rss</guid>
<pubDate>Sat, 25 Jul 2020 23:19:16 -0500</pubDate>
<description><![CDATA[
<h2 style="text-align: center"><a href="https://simonxiang.xyz/rss.xml">My RSS Feed</a></h2>

<p>
The best way to keep up with what I'm doing is to periodically check for updates, much like how you periodically run <code>pacman -Syu</code> on your Arch Linux machine. However, the more people you keep up with and the more individual websites you have to visit, the more of a pain this process becomes. Enter RSS feeds.
</p>

<h3>Why Use RSS?</h3>

<p>
RSS feeds functions like social media without the bad parts. Rather than having information centralized in one place (and therefore susceptible to being controlled), all you have to do is add a bunch of RSS feeds (urls) to your RSS reader of choice, and it will aggregate the content for you for easy viewing. This way, you don't have to individually check a bunch of sites, all you have to do is refresh your RSS reader and it will pull all the updates to the feeds you've subscribed to automatically.
</p>

<p>
Of course, it's completely private since all RSS readers do is check for updates to a certain url on someone's page. You can use RSS feeds to replace all sorts of things! For example, you can find RSS feeds for 

<ul type="disc">
<li type="disc">Youtube Channels</li>
<li>Individual Subreddits</li>
<li>Twitter Feeds</li>
<li>Public Facebook Pages</li>
<li>Instagram Feeds</li>
<li>Github Repository Updates</li>
</ul>

and of course, blogs! No longer will you have to subject yourself to involuntary surveillance if all you wanted to do was see what someone is up to— which was the original premise of social media anyway, before the mass centralization of internet content. 
</p>

<h3>Recommendations for an RSS Feed Reader</h3>

<p>
The RSS reader I'm currently using is <a href="https://wiki.archlinux.org/index.php/Newsboat">Newsboat</a> (terminal based, of course), with vim bindings from Luke Smith's config. I'm currently still working on setting everything up, but you can view my progress on <a href="https://github.com/simonxiang1/dotfiles/tree/master/.config/newsboat">GitHub</a> or my <a href="https://git.simonxiang.xyz/dotfiles/file/.config/newsboat/config.html">Git server.</a> Note that for Newsboat to start, you'll need to add at least one RSS feed to your <code>~/.config/newsboat/url</code> file. 
</p>

<p>
Of course, if you're not a fan of terminal based applications, there are graphical RSS readers as well. I haven't used any of them, but the popular choices seem to be <a href="https://www.ghacks.net/2019/07/06/newsflow-is-a-free-customizable-rss-reader-app-for-windows-10/">Newsflow (Windows)</a> and <a href="https://jangernert.github.io/FeedReader/">FeedReader (Unix based systems)</a>.
</p>
]]></description>
</item>








<item>
<title>I'm Never Going to Finish Reading Infinite Jest</title>
<guid>https://simonxiang.xyz/blog/im-never-going-to-finish-reading-infinite-jest</guid>
<pubDate>Fri, 24 Jul 2020 22:25:32 -0500</pubDate>
<description><![CDATA[
<p>
The year is 2157. I'm sitting in a room with a book in my hand. No, the room is the book. If you look closely, you can make out the slight imprint of the faded words <span style="font-size: 14px">"Infinite Jest"</span> lining the walls, miniscule page numbers spanning the contents like hieroglyphs. We turn our attention to the rest of the "room". On the ceiling is emptiness: there is no light, or any hope of one. In the corner lie my other books, covered with a thick layer of dust. I desperately reach for my copy of Murakami's <em>Kafka on the Shore</em>, grasping at anything that can bring me comfort. Maybe Nakata can lead me out of this place, to the entrance stone and into an alternate reality. Actually, at this point, I would even be OK with Johnny Walker. I brush off the layer of dust, only to reveal another layer of dust. I open the book— it's all dust. <em>"Unimportant."</em> David Foster Wallace is staring at me from the ceiling. Of course, I should have known. I take my Sailor 1911 and dip it in the ink from the pages of the room, and use it to erase the middle third of the floor. We are on the 141st iteration of the Cantor Ternary set, and I have made a total of $2^{141}-1$ deletions, which follows from a quick proof by induction.
</p>

<p>
Of course, I am already dead, along with all my descendants, their descendants, and their descendants, all the way down to the $10^{38}$th generation. As you can probably tell by now, the year is not 2157. All $e^{e^{87.5}\ln{(2)}}$ of my descendants are cursing me for the day I decided to pick up a copy of <em>Infinite Jest</em> because I heard that David Foster Wallace structured it like the Sierpinski Gasket and included a two-page footnote on the MVT. I reach for the pen— there is no pen. David Foster Wallace is looking into my soul, which is quite easy to do since my body has long since disintegrated away. I too, look into my soul, and see a fine print lightly embroidered along the edges. <span style="font-size: 12px">"Infinite Jest"</span>, it reads. Once again, David Foster Wallace reminds me that there is no hope. As I turn of a page of the wall, a small television in the corner of the ceiling flickers and updates. The television is watching my every move. "40%", it reads. I resign myself to my fate and erase another third of the floor.
</p>
]]></description>
</item>










<item>
<title>Linear Algebra and Its Applications, Part 1: An Introduction</title>
<guid>https://simonxiang.xyz/blog/linear-algebra-and-its-applications-part-1-an-introduction</guid>
<pubDate>Thu, 23 Jul 2020 22:04:17 -0500</pubDate>
<description><![CDATA[
<p>
Yes, the title refers to that one horrid textbook that everybody uses. I hope that reading the title brought back some fond memories of your first Linear Algebra course!
</p>

<p>
Welcome to my new series on the many applications of Linear Algebra! I'm not exactly sure which applications I'll cover or how many just yet, but for now I'll just go with the flow. I'm also not much of an applied mathematician, so I'll probably end up writing about the applications of Linear Algebra in other fields of math or something like that. 
</p>

<p>
Linear Algebra pops up seemingly everywhere— if we rephrase "systems of linear equations" as "many equations that seem somewhat straight if you zoom in", it becomes clear why. Here's an inspirational quote:
</p>

<blockquote style="font-style: italic">
<p>
"Mathematics is about turning difficult problems into Linear Algebra problems."
</p>
<cite style="font-style: normal">–Terence Tao</cite>
</blockquote>
<p>
Actually, it probably wasn't Terence Tao who said that. I probably got the quote wrong too. Anywho, you get the point. 
</p>

<p>
Matrix equations will show up all the time if you work in a STEM field, and about half of modern mathematics is built on the theory of vector spaces. It's a fundamental topic everywhere (and should be taught in high schools!) that deserves some special treatment, and so, the birth of this series. Look forward to my first post on image compression with Singular Value Decomposition!
</p>
]]></description>
</item>















<item>
<title>How to Contact Me</title>
<guid>https://simonxiang.xyz/blog/how-to-contact-me</guid>
<pubDate>Mon, 20 Jul 2020 22:14:09 -0500</pubDate>
<description><![CDATA[
<h2 style="text-align: center">Email Addresses</h2>

<li style="text-align: center; font-size: 21px"><a href="mailto:simon@simonxiang.xyz">simon@simonxiang.xyz</a></li>
<li style="text-align: center; font-size: 21px"><a href="mailto:simonxiang@utexas.edu">simonxiang@utexas.edu</a></li>
<li style="text-align: center; font-size: 21px"><a href="https://simonxiang.xyz/examples/simonspublickey.gpg">My Public GPG Key</a></li>

<h3>On Cell Phones</h3>

<p>
If you know me in real life, you probably know my cell phone number or someone that has my cell phone number. The quickest way to contact me is by call. I won't respond to texts quickly because my phone doesn't alert me if someone texts— by nature they convey less important information and are more prone to sucking you into a spiral of distraction. I have many other grievances with text messages that I will not air here. If you do insist on using text, it would be nice if you used <a href="https://signal.org">Signal</a> though.
</p>

<h3>On Email</h3>

<p>
The second best way to contact me is by email, which you can find at the top of this post. I've also attached my public GPG key if you want to send me encrypted emails, you can <a href="https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-1-asymmetric-encryption">read</a> <a href="https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-2-fermats-little-theorem-and-ring-theory">my</a> <a href="https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-3-digital-signatures-and-eulers-totient-function">posts on RSA Encryption</a> if you don't know what this is. If you want to contact me for academic purposes, I've listed my UT email address as well. Be aware that Gmail is the email provider for UT, so don't include anything that you wouldn't put on a Times Square billboard in any email you send to my UT email address.
</p>

<p>
If you have the time, I would like you to read <a href="https://www-cs-faculty.stanford.edu/~knuth/email.html">Donald Knuth's epic stance on email</a>. I too, make an effort to be on the bottom of things— although I may not be as cool as Knuth, doing mathematics still requires intense concentration for long periods of time. The average office worker checks their email once every six minutes. If a mathematician was as easily distracted, they wouldn't be able to prove a single thing all day.
</p>

<h3>On Social Media</h3>

<p>
Something I do not have is social media of any kind, including Facebook, Instagram, Reddit, Linkedin, etc. There are a plethora of reasons not to use these services and the reasons to use them are few and far between. Setting the strongest case for not using social media aside (privacy), they cause massive damage in other areas of your life by the concept of instant gratification through dopamine hits. 
</p>

<p>
Everytime you refresh a Facebook page, you're pulling the lever for a slot machine, anticipating the possible arrival of a shiny new burst of dopamine. Your phone will light up at every new like, follow, or text message, all bringing successive and random hits of dopamine. This conditions you to check your phone as often as possible in order to maximize the chance of receiving the next hit sooner. As expected, this is disastrous for our brains and our ability to concentrate and produce meaningful work. If you want to read more on this topic, I highly suggest reading <em>Digital Minimalism</em> by Cal Newport.
</p>

<h3>In Short</h3>

<p>
My email is at the top of the page, and if you know me you most likely have my phone number. Response times will be slow, but it's not a personal thing— if I respond quickly, I'm probably not doing what I'm supposed to be doing. I don't use social media and you shouldn't either— hating social media isn't a boomer exclusive mindset. Finally, thanks for reading this and have a nice day. 
</p>
]]></description>
</item>










<item>
<title>Digital Privacy is Like the Blinds on Your Windows</title>
<guid>https://simonxiang.xyz/blog/digital-privacy-is-like-the-blinds-on-your-windows</guid>
<pubDate>Sun, 19 Jul 2020 14:19:23 -0500</pubDate>
<description><![CDATA[
<p>
When I'm at home, I prefer to keep my blinds down, unless the Sun is hitting my window at a good angle. It gives me a sense of security and calm to know that I can do whatever I want without the possibility of anyone scrutinizing my actions. I do things like reading books and taking showers behind the safety of my blinds— it's not like these are inherently immoral acts that need to be kept secret, thus justifying my use of blinds. I just like to be left alone when I'm minding my own business.
</p>

<p>
Some people like keeping their blinds open. They roll them all the way down and proudly declare "I have nothing to hide! Why should I keep my blinds down?" They tell me that trying to keep your blinds down is a waste of time, since you've had them open your entire life anyway. Besides, what's the point of lowering the blinds when you're already being watched? They have a point— their house is made of glass. Everything is transparent from the outside, all the way to their bathroom doors on the second floor.
</p>

<p>
Of course, it is not their fault for buying a transparent house. The three real estate giants, known as Goggle, Pear, and Megasoft have been pushing to make transparent houses an industry standard for years. They market the houses as the "homes of the future", appealing to the consumer using flashy features like "Fingerprint unlock the front door!", "Higher resolution security camera!", and "Better integration with our Smart typewriter and fan!" Of course, they designed the typewriter and fan to fail lest you not have a glass house.
</p>

<p>
At this point, buying a glass house has been normalized. People are waiving away their right to keep the blinds down left and right, for the sake of faster popcorn delivery by drone and convenience. This is not their fault either— the Big Three real estate companies have made it very difficult to find out what you are actually agreeing to when you sign the contract for your new glass house. On the contrary, it is very easy to sign the contract itself, almost too easy, in fact.
</p>

<p>
You didn't know about the microphone planted in your car, the cameras in the corners of the rooms. The smart fridge sends a report of your weekly groceries to Megasoft. The chair is analyzing the weight of each person that sits on it and sends it over to Goggle for "analytics". Of course, all the electronics are always on and recording by default. You have the option to turn them off individually, but there are so many of them that this is a gargantuan task. You don't even have the option to remove them from your house completely. Of course, some have tried prying them from the walls, only to see them reinstalled following the next mandatory "maintanence update". You don't choose when the maintanence updates happen either— the mechanics force open the door, kick you out, and make you sit on your front lawn until they're done.
</p>

<p>
Goggle, Pear, and Megasoft get away with this by not releasing the blueprints to the homes they design. Nobody can really prove the extent that the Big Three spy on us, because their blueprints are private and closely guarded. They have the ability to implement whatever kind of tracking device they want at the drop of a hat, because you don't have the ability to look into the inner workings of your house and decide whether you like what they are doing or not. You signed that right away when you signed up to use the house.
</p>

<p>
Oh, I almost forgot about the tracking device by Pear that is attached to your person at all times. On average, people will spend 5 hours of their day looking at the device, because Pear designed the device to be addictive. It serves the same purpose as an ankle bracelet attached to prisoners, but it sends a little bit more than just your location.  It sends your conversations with friends, sleeping hours, personal library, video game data, everything— it sends it all to Pear. Not only is it virtually impossible to disable the tracking "services", most people actually signed up for this one willingly.
</p>

<p>
Privacy is a fundamental right, not a privilege. With each subsequent invasion of our privacy, the possibility of an Orwellian totalitarian surveillance state becomes more and more real. The government is very happy that we continue to sign our privacy rights away, and is very upset when we resist— just look at what the NSA did to Snowden, and what they're trying to do to the Tor developers. Our world is truly screwed the day building brick-and-mortar houses and closing your blinds becomes illegal. Join me in living in a house that has an open-source blueprint and closing the blinds to stop the peering eyes of The Big Three. Join me in fighting for the right to digital privacy.
</p>
]]></description>
</item>


<item>
<title>Regrettably, This Site Runs Javascript</title>
<guid>https://simonxiang.xyz/blog/regrettably-this-site-runs-javascript</guid>
<pubDate>Sat, 18 Jul 2020 17:21:49 -0500</pubDate>
<description><![CDATA[
<p>
I'm really sad. I had a vision for a completely bloat-free website, but now I'm being plagued by the blight that is Javascript because I wanted pretty equations. Every single time someone loads my rolling blog page, it sends 40 something calls to the external MathJax server to generate the LaTeX equations, slowing down the load time by one or two seconds. Unacceptable! I want my stuff to load instantly all the time.
</p>

<p>
I've tried to host an instance of MathJax on my server before, but that means installing a bunch of bloat like Node.js on the server end. Plus, I'm not even sure it'll improve response times by that much. Image based solutions look absolutely horrible. Is there any clean solution to writing pretty equations in HTML? If anybody knows of one, please let a poor and struggling mathematician know.
</p>

<p>
Edit: An additional grievance: <code>\overbrace</code> and <code>\underbrace</code> will break MathJax. It doesn't even compile the rest of your document if you have one of these bad boys in your code! Someone please save me.
</p>
]]></description>
</item>


<item>
<title>Proving RSA Encryption: An Application of Group Theory (Part 3: Digital Signatures and Euler's Totient Function)</title>
<guid>https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-3-digital-signatures-and-eulers-totient-function</guid>
<pubDate>Fri, 17 Jul 2020 18:51:10 -0500</pubDate>
<description><![CDATA[
<p>
We've finally proved Fermat's Little Theorem and explained some of the machinery behind groups and rings. Let's continue by defining an important function.
</p>
<p><b><u>Definition</u></b><em> (Euler's Totient Function):</em> Let $\varphi: \mathbb{Z}^+ \to \mathbb{Z}^+$ be defined as $\varphi(n) = $ the number of integers less than or equal to $n$ that are relatively prime to $n$ for $n \in \mathbb{Z}^+. \varphi$ is also known as the <b>Euler phi-function</b>.
</p>

<p>
As you can see, it can be quite hard to procure a general formula for $\varphi(n)$ for all $n$. However, in some cases it is relatively easy— for example, if $n$ is prime, then $\varphi(n)$ is simply $n-1.$ Recall from the last post that the Euler phi-function simply defines the <em>order</em> of the multiplicative group of units $\mathbb{Z}_n^*.$ We now state an important theorem:
</p>
<p><b><u>Theorem</u></b><em> (Euler's Theorem):</em> Let $a \in \mathbb{Z}$ be relatively prime to $n,$ that is, $\text{gcd}(a,n)=1.$ Then 
\[
a^{\varphi(n)} \equiv 1 \, (\text{mod} \, n),
\]
where $\varphi$ denotes the <em><b>Euler phi-function,</b></em> and $n \in \mathbb{Z}^+.$
</p>
<p><b><u>Proof:</u></b> Euler's Theorem is equivalent to the following statement: For all $a \in \mathbb{Z}_n^*$, $a^{\varphi(n)} \equiv 1$ (mod $n$). We know from a previous theorem that any element of a finite group raised to the power of the order of the group is $1.$ Since $|\mathbb{Z}_n^*|=\varphi(n)$, we have 
\[
a^{\varphi(n)} \equiv a^{|\mathbb{Z}_n^*|} \equiv 1 \, (\text{mod} \, n). \quad \boxtimes
\]
</p>

<p>
Notice that the reasoning behind the proof is almost the exact same as the proof of Fermat's Little Theorem: <b>Indeed, Euler's Theorem is simply a <em>generalization</em> of Fermat's Little Theorem—</b> Fermat's Little Theorem simply describes the case where $n$ is prime.
</p>
<p>
Let's calculate $\varphi(n)$ for $n=pq$, where $p$ and $q$ are two prime numbers. We want to find the number of integers less than or equal to $n$ that are relatively prime to $n$. First, note that the number of integers less than or equal to $n$ is $n-1,$ or $pq-1$. We want to subtract the number of integers that have a multiple in common with $n.$ This is what we have so far: 
\[
\varphi(n) = (pq - 1) - ? - ?
\]
 Since $n$ is composed of two prime numbers, this would entail the multiples of $p$ and $q$ (due to the unique factorization of $n$) that are less than $n$, since $n=pq$ is already accounted for. There are $p-1$ multiples of $q$ and $q-1$ multiples of $p$ that are less than or equal to $n$. This makes sense if you think about $n$ this way. Without loss of generality,
 \[
 n = pq = q + \cdots + q
 \]
 $p$ times. Then the distinct numbers
 \begin{align}
     &1:q \\
     &2:q + q \\
     &3:q + q + q \\
     & \vdots \\
     &p-1:q + \cdots + q 
 \end{align}
 are all multiples of $q$ that are less than $n$, hence why we stopped at the term $p-1$. Our goal is find the <em>number</em> of factors of $q$: then there must be $p-1$ factors since the set the set $\{1, 2, 3, \cdots p-1 \}$ has $p-1$ elements. A similar argument holds for the factors of $p$. We have some more information on how $\varphi(n)$ looks now...
 \begin{align}
     \varphi(n) &= (pq - 1) - (p - 1) - (q - 1) \\ 
                &= pq - p - q + 1 \\ 
                &= p(q-1)-(q-1) \\ 
                &= (p-1)(q-1).
 \end{align}
...Oh yeah, it's all coming together. 
</p>

<p>
Now we finally understand the reasoning behind choosing the seemingly arbitary number $(p-1)(q-1)$: it's the <em><b>order of the multiplicative group of integers mod $pq$</b></em>, which means <em><b>any element of $\mathbb{Z}_{pq}$ will reduce to 1 if raised to the power of $(p-1)(q-1)$ modulo $(p-1)(q-1)$ by Euler's Theorem.</b></em> Let us finally prove that pesky lemma from the first post.
</p>
<p><b><u>Lemma:</u></b> Let $n=pq$, where $p$ and $q$ are two distinct primes. If $a$ is an integer relatively prime to $n$ and $w$ is an integer such that $w \equiv 1 \, \text{mod} \, (p-1)(q-1)$, then 
\[
a^w \equiv a \, (\text{mod} \,n).
\]
</p>
<p><b><u>Proof:</u></b> Since $w \equiv 1 \, \text{mod} \,(p-1)(q-1), w = k(p-1)(q-1)+1$ for some $k \in \mathbb{Z}$. So
\begin{align}
    a^w &= a^{k(p-1)(q-1)+1} \\
        &= a^{(p-1)(q-1)^k}a \\
        &= a(a^{\varphi(n)})^k \\ 
        &\equiv a(1)^k \ \small{\text{by Euler's Theorem}} \\
        &= a \, (\text{mod} \, n).
\end{align}
</p>

<p>
With that, we've finally finished the theory behind RSA Encryption. Let's end this series with a real-world application, follow along on your Linux machines everyone! (Disclaimer: This should work on any machine with GPG installed).
</p>

<p>
We can use RSA Encryption to digitally sign messages, that is, generate a signature that anybody with my public key can decrypt, but only I can encrypt. Go ahead and <a href="https://gnupg.org/download" target="_blank">grab yourself of a copy of GnuPG</a>, the program we're going to use to create keys and verify signatures. If you're on Linux, just run 

<pre>
<code>sudo pacman -S gnupg</code>
</pre>

or some variant (if you're using Linux, I hope you know how to install packages).
</p>

<p>
We'll use the same notation as the original post, refer back to it if you need to. Go ahead and download <a href="https://simonxiang.xyz/examples/simonspublickey.gpg">my public key,</a> <a href="https://simonxiang.xyz/examples/hello_world.txt">the message we're going to be verifiying,</a> and <a href="https://simonxiang.xyz/examples/hello_world.sig">the digital signature generated by RSA Encryption.</a> 
</p>

<p>
The algorithm starts by encoding $m$ (hello_world.txt) as an integer. We generate the digital signature by using my <em><b>private key</b></em> and letting the singature equal 
\[
m^s \, (\text{mod} \, n).
\]
The resultant signature is encoded in the file hello_world.sig above. 
</p>

<p>
If you want to generate a GPG key pair and sign a file of your own, go ahead and open your terminal emulator and run 
<pre>
<code>gpg --gen-key</code>
</pre>
and follow the instructions. To generate the digital signature for a file, run 
<pre>
<code>gpg --output file.sig --detach-sig file</code>
</pre>
and replace <code>file.sig</code> with whatever name you want your signature file to be, and <code>file</code> with the file you want to sign. 
</p>

<p>
Now since I used my <em><b>private key</b></em> to sign this file, you'll need my <em><b>public key</b></em> to verify the signature. <code>cd</code> to the directory where you downloaded my public key, and run 
<pre>
<code>gpg --import simonspublickey.gpg</code>
</pre>
to add my public key to your <em><b>keyring</b></em>.
</p>
<p>
We verify the signature works by raising it to the power of $r$ (which is in the public key) to yield 
\[
(m^s)^r \equiv m^{rs} \equiv m \, (\text{mod} \, n)
\]
by our lemma. Isn't that neat? Basically, it compares both the message $m$ and the signature raised to the power of $r$, denoted $(m^s)^r$, and checks to see if the two match— if they do then great, if they don't, then the signature has been compromised/the message has been tampered with since it was sent, and should not be trusted.
</p>

<p>
To follow along on your own machine, <code>cd</code> to the directory you downloaded the files in and run
<pre>
<code>gpg --verify hello_world.sig hello_world.txt</code>
</pre>
Look at the third line of the output, which should read 
<pre>
<code>gpg: Good signature from "Simon ... " [unknown]</code>
</pre>
If it does, then we've successfully verified the digital signature using the steps described. We conclude that I am indeed the author of the message, and that it hasn't been tampered with since publication. (You can ignore the "<code>WARNING: This key is not certified with a trusted signature!</code>" message, it just means I haven't signed my GPG key yet.) 
</p>

<p>
And that's a wrap! If you want to send me encrypted emails, you have my public key now so feel free to do so. Together, we can use the power of math and encryption to protect our content from the peering eyes of the NSA!
</p>
]]></description>
</item>












<item>
<title>Proving RSA Encryption: An Application of Group Theory (Part 2: Fermat's Little Theorem and Ring Theory)</title>
<guid>https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-2-fermats-little-theorem-and-ring-theory</guid>
<pubDate>Fri, 17 Jul 2020 16:19:15 -0500</pubDate>
<description><![CDATA[
<p>
This post is going to be a little heavy on the group theory— I'm going to try to build everything from the ground up but having a prior understanding of the basics of groups will be very useful. Let's start by stating an important theorem:
</p>
<p><b><u>Theorem</u></b><em> (Fermat's Little Theorem):</em> Let $a \in \mathbb{Z}$, $p$ a prime. If $p$ doesn't divide $a$, then $p$ divides $a^{p-1} - 1$, that is, 
\[
a^{p-1} \equiv 1 \, (\text{mod} \, p).
\]
Those with a background in number theory may be more familiar with this equivalent statement of the theorem: If $a \in \mathbb{Z}$, then $a^p \equiv a \, (\text{mod} \, p)$ for $p$ a prime.
</p>
<p><b><u>Proof:</u></b><em> I have discovered a truly remarkable proof that is too large to be contained in the margins of this post.</em> 
</p>

<p>
Just kidding. Before we prove Fermat's Little Theorem, let's talk about groups and fields. Here's how groups are defined (I'm going to let addition denote an arbitrary binary operation):
</p>
<p><b><u>Definition:</u></b> A <b><em>group</em></b> $\langle G,+ \rangle$ is a set $G$ closed under addition such that 
<li>&emsp; $\mathscr{G}_1$ (Associativity): For all $a, b, c \in G,$ 
\[
(a+b)+c = a+(b+c)
\]
</li>
<li>$&emsp; \mathscr{G}_2$ (Identity): There exists an element $0 \in G$ such that for all $a \in G$, 
\[
0+a=a+0=a.
\]
</li>
<li>$&emsp; \mathscr{G}_3$ (Inverse): For all $a \in G$, there exists an $a' \in G$ such that 
\[
a + a' = a' + a = 0.
\]
</li>
</p>

<p>
The cool thing about groups is that they're very simple yet powerful: as long as <em><b>any set</b></em> paired with <em><b>any binary operation</b></em> satisfies these axioms, it forms a group, and all the theorems from group theory apply to it. It doesn't just have to be $\mathbb{R}$ with the standard addition, anything from the set of square matrices $(GL(n, \mathbb{C}))$ to the set of permutations on a regular $n$-gon $(S_n)$ form a group.
</p>

<p>
Let's prove a theorem that will come in handy later. Since this post is going to be very technical, I'm not going to explain the proof since I'll have to explain everything that builds onto the main theorem used in the proof, Lagrange's Theorem.
</p>

<p><b><u>Theorem:</u></b> Let $G$ be a finite group under multiplication (so 1 is now the identity), and let $|G|$ denote its cardinality (order). Then for all $a \in G$, 
\[
a^{|G|} = 1.
\]
</p>
<p><b><u>Proof:</u></b> By <em>Lagrange's Theorem</em>, the order of the element $a$ (the smallest positive integer $k$ such that $a^k=1$) must divide $|G|$, that is, $|G|=kn$ for some $n \in \mathbb{N}$. Then 
\[
a^{|G|}=a^{kn}=(a^k)^n=1^n=1. \quad \boxtimes
\]

</p>

<p>
Let's take a look at the specific group $\mathbb{Z}_p$, consisting of the set of integers 
\[
\{0,1,2, ... , p-1\}
\]
for $p$ a prime, paired with the binary operation of modular arithmetic. For example, $5+6$ (mod 7) $\equiv 11$ (mod 7) $\equiv 4$. I'll leave it to you to check that this forms a group. We can actually go further: we can tack on a second binary operation (modular multiplication) and call it a <em><b>ring</b></em>. 
</p>

<p>
Rings have many properties, but the important ones are that multiplication is associative, follows the distributive law ($a*(b+c) = a*b + a*c$), and that the ring has an identity for multiplication (there exists an element $1 \in R$ such that $1*a=a*1=a$ for all $a \in R$). Formally, a ring that has an identity element is called a <em>ring with unity</em>.
</p>

<p>
The thing about rings is that <em>not every element has a multiplicative inverse</em> like groups do. For example, take the element $0$. Since $0+0=0$, $a*0=a*(0+0)=a*0+a*0$ for all $a \in R$. If $a*0=a*0+a*0$, just add the additive inverse of $a*0$ to both sides of the equation to yield $0=a*0+0=a*0$. So $a*0 = 0$ for all $a \in R$. Now if $0$ had a multiplicative inverse, that's saying that there exists some element $0' \in R$ such that $0'*0=1$, which is a contradiction since we would have both $0'*0=0$ and $0'*0=1$. Therefore $0$ cannot have a multiplicative inverse.
</p>

<p>
Elements of rings that <em>do</em> have multiplicative inverses are called <em><b>units</b></em>. We have another handy theorem that lets us classify the units in $\mathbb{Z}_p$! However, I'm going to skip explaning the machinery behind the proof for the same reasons as the first proof.
</p>
<p><b><u>Theorem:</u></b> Let $n$ be the order of the ring $\mathbb{Z}_n$ and $m \in \mathbb{Z}_n$. Then if $\text{gcd}(m,n)=1, m$ is a  <em><b>unit</b></em>.  
</p>
<p><b><u>Proof:</u></b> Since $\text{gcd}(m,n)=1$, for some $a, b \in \mathbb{Z}$, we have $an+bm=1$ by <em>Bezout's Identity.</em> By the <em>Division Algorithm</em>, we know there exist integers $q$ and $r$ such that $0 \leq r \leq n-1$ and $b=nq+r$. Then 
\[
rm = (b-nq)m = bm-nqm = (1-an)-nqm = 1 - n(a+qm).
\]
Since we live in the ring $\mathbb{Z}_n$, multiplication is commutative and multiples of $n$ will reduce to $0$ (mod $n$). Therefore $n(a+qm) \equiv 0 $ (mod $n$), and $rm=mr=1$. Since we have found a multiplicative inverse for $m \in \mathbb{Z}_n$, we conclude that $m$ is a unit. $\quad \boxtimes$
</p>

<p>
There are special rings in which <b>every non-zero element is a ring</b> called <em><b>fields</b></em>. Intuively, fields are algebraic structures in which <em>division </em> is legal, since every element is a unit and therefore will remain in the field if "divided by" (multiplied by the multiplicative inverse) of another element in the field. From the previous theorem, the wonderful fact that $\mathbb{Z}_p$ is a field for $p$ a prime follows! Notice that every non-zero element of $\mathbb{Z}_p$ is relatively prime to $p$ (that is, gcd$(m,p)=1$ for $m \in \mathbb{Z}_p$) since $p$ is a prime number, so every non-zero element of $\mathbb{Z}_p$ is a unit, hence $\mathbb{Z}_p$ is a field.
</p>

<p>
Algebra is all about analyzing <em>structure</em>. Like a phoenix rising from its ashes, a group emerges from this field. We claim that the <em><b>set of units in $\mathbb{Z}_p$ form a group</b></em> for $p$ a prime. The hardest part about showing that this is a group <b>has already been done!</b> That is, showing that every element has an inverse, which is trivial since every element is a unit.
</p>

<p>
<strong>We are ready to prove Fermat's Little Theorem</strong>. Before we do, note that $p$ doesn't necessarily have to be prime for the set of units in $\mathbb{Z}_p$ to from a group. However, since $p$ is prime, we know that every non-zero element is a unit, and therefore we can easily classify this group as consisting of the elements 
\[
\{1,2,3, ... ,p-1\}
\]
and having order $p-1$. Let's denote our new group as $\mathbb{Z}_p^*$. For convenience, we'll restate Fermat's Little Theorem.
</p>
<p><b><u>Theorem</u></b><em> (The Little Theorem of Fermat):</em> Let $a \in \mathbb{Z}$, $p$ a prime. If $p$ doesn't divide $a$, then $p$ divides $a^{p-1} - 1$, that is, 
\[
a^{p-1} \equiv 1 \, (\text{mod} \, p).
\]
</p>
<p><b><u>Proof:</u></b> We can restate Fermat's Little Theorem as such: For all $a \in \mathbb{Z}_p^*$, $a^{p-1} \equiv 1$ (mod $p$). Recall a previous theorem: <em>If $G$ is a finite group under multiplication, then $a^{|G|}=1$ for all $a \in G$.</em> Clearly $\mathbb{Z}_p^*$ is a finite group under multiplication, and the order of $\mathbb{Z}_p^*$ is $p-1$, that is, $|\mathbb{Z}_p^*|=p-1$. Therefore, for all $a \in \mathbb{Z}_p^*$,
\[
a^{p-1} \equiv a^{|\mathbb{Z}_p^*|} \equiv 1 \, (\text{mod} \, p). \quad \boxtimes
\]
</p>

<p>
(For the curious reader: Here is the reasoning behind why the two statements of FLT are equivalent. Every integer will correspond to a coset of the quotient ring $\mathbb{Z}/p\mathbb{Z}.$ In other words, if $b \in \mathbb{Z}$, then $b \in a+p\mathbb{Z}$ for some $0 \lt a \leq p-1$ $(a \in \mathbb{Z}/p\mathbb{Z}),$ which implies $a+p\mathbb{Z}=b+p\mathbb{Z}$ by the definition of cosets. So we can restate FLT as such: $\forall b+p\mathbb{Z}, (b+p\mathbb{Z})^{p-1} \equiv 1$ (mod $p$) which implies $\forall a+p\mathbb{Z}, (a+p\mathbb{Z})^{p-1} \equiv 1$ (mod $p$). Since $\mathbb{Z}/p\mathbb{Z}$ and $\mathbb{Z}_p$ are naturally isomorphic as rings (similarily, $\mathbb{Z}/p\mathbb{Z}^*$ and $\mathbb{Z}_p^*$ as groups), this statement is the same as $\forall a \in \mathbb{Z}_p, a^{p-1} \equiv 1$ (mod $p$). Therefore, FLT is equivalent to the alternate statement.) 
</p>

<p>
Once again, this post has grown quite long. I'll wrap this series up in the next post, look forward to it!
</p>
]]></description>
</item>








<item>
<title>Proving RSA Encryption: An Application of Group Theory (Part 1: Asymmetric Encryption)</title>
<guid>https://simonxiang.xyz/blog/proving-rsa-encryption-an-application-of-group-theory-part-1-asymmetric-encryption</guid>
<pubDate>Fri, 14 Jul 2020 16:33:15 -0500</pubDate>
<description><![CDATA[
<p>
If someone receives an email from me, they might notice a peculiar "signature.asc" file attached at the bottom. This is not a bug— this is so the recipient can decrypt this file with the <b>RSA encryption algorithm</b> and verify that the sender is in fact, me, and the message has not been tampered with.
</p>

<p>
In essence, how the RSA algorithm works is by multiplying two very large prime numbers, known only to the recipient of the message, and making the product public (this is known as a <b><em>public key</em></b>). The sender uses the product to encrypt a message using an algorithm that only someone with knowledge of the two prime numbers (known as the <b><em>private key</em></b>) can decrypt. We can leave the question of the computational difficulty of integer factorization, i.e., whether or not there exists an algorithm that factors large prime numbers in polynomial time, to the computer scientists (hopefully there isn't, or else a large portion of modern day encryption becomes useless). 
</p>

<p>
What we're concerned with, as mathematicians, is the <b>validity</b> of the algorithm. We want to know, given <b><em>any</em></b> message, will the recipient <b><em>always</em></b> be able to decrypt the encrypted message with their private key and get the right message? Let's get right into the math— all you'll need to be familiar with is some basic number theory.
</p>

<p><b><u>Definition:</u></b> Let $n$ be the product of two prime numbers $p$ and $q$ respectively, and $s$ be an integer relatively prime to $(p-1)(q-1)$. Let $p, q,$ and $s$ be part of the <em><b>private key</b></em>, and let $n$ and $r$ be part of the <em><b>public key</b></em>, where $r$ is the <em>multiplicative inverse</em> of $s$ mod $(p-1)(q-1)$. That is, $rs \equiv 1$ mod $(p-1)(q-1)$.  
</p>

<p>
The definitions may seem a little strange and arbitrary— no worries! Everything will fall into place over time. For now, our goal is to show that, using the public and private keys, the sender and recipient can <em>successfully complete an <b>encryption/decryption exchange</b></em>. Let's define what that is real quick: in this case, let $m$ be the message being encrypted (Note that $m$ is an integer— there are several ways to encode messages as integers that we will not cover here for the sake of brevity).       
</p>

<p><b>Encryption:</b> Let $e$ equal the encrypted message, which is determined by 
\[
e \equiv m^r \, (\text{mod} \, n).
\]
This is simple since $n$ and $r$ are known, and a computer can easily perform modular exponentiation in polynomial time.
</p>

<p>
<b>Decryption:</b> After receiving the encrypted message $e$, we decrypt it by computing 
\[
e^s \equiv (m^r)^s \equiv m^{rs} \equiv m \, (\text{mod} \, n).
\]
We can do this because $s$ is part of the <em><b>private key</b></em>. Note that in this case, we're dealing with a form of <em><b>asymmetric encryption</b></em>, since <b>anyone with the public key</b> can encrypt a message but <b>only the owner of the private key</b> can decrypt it. Everything makes sense up until the last implication. How does $m^{rs} \equiv m \, (\text{mod} \, n)$?
</p>

<p>
We need a lemma.
</p>

<p><b><u>Lemma:</u></b> Let $n = pq$, where $p$ and $q$ are two distinct primes. If $a$ is an integer relatively prime to $n$ and $w$ is an integer such that $w=1$ mod $(p-1)(q-1)$, then 
\[
a^w \equiv a \, (\text{mod} \, n).
\]
</p>
<p><b><u>Proof:</u></b> <em>Left as an exercise to the reader.</em> 
</p>

<p>
Just kidding. This lemma is built off a powerful theorem, whose proof relies heavily on group theory. Since this post is getting rather long, I'll prove it in the next post, as well as shed some light on the intuition behind choosing the oddly specific number $(p-1)(q-1)$ for $s$ to be relatively prime to. Now that we have this lemma, the equivalence 
\[
m^{rs} \equiv m \, (\text{mod} \, n) 
\]
follows from the fact that we defined $r$ to be the <em>multiplicative inverse </em> of $s$ (so $rs \equiv 1$ mod $(p-1)(q-1)$), and the lemma above, setting $a$ equal to $m$ and $w$ equal to $rs$. 
</p>

<p>
And there we have it! Look out for part two, where I'll talk about digital signatures, group theory, and implementing this in real life.
</p>
]]></description>
</item>

















<item>
<title>Why does my website look so old?</title>
<guid>https://simonxiang.xyz/blog/why-does-my-website-look-so-old</guid>
<pubDate>Sun, 12 Jul 2020 15:13:05 -0500</pubDate>
<description><![CDATA[
<p>Here are some reasons why.</p>

<h3>The web is bloated</h3>

<p><strong>In a world of overdesigning and excessive emphasis on the graphical user experience, the internet has become massively bloated.</strong> How many times have you loaded up a website for it to look like <a href="pictures/bloated_wire.jpg" target="_blank">this?</a> <a href="http://everyfuckingwebsite.com" target="_blank">Or what about this?</a> (I don't condone swearing by the way).</p>

<p> Either way, "modern" websites have become an abomination. The original premise that websites should be distributed, accessible, and convey information effectively has been abandoned in favor of advertisements, bloat, and rampant censorship. 
</p>

<h3> Self-sufficiency on the Internet</h3>

<p>When I set out to create this website, I had two goals, the first of which was <strong>self-sufficiency.</strong> I own the VPS this is hosted on and set up the SSL certificate/nginx myself (with the help of some open source software). I write blog posts in HTML and upload them without the help of a static site generator. 
</p>

<p>Sure, I gain some functionality and save some time if I host my site with GitHub or use Wordpress as a backend for my blog. <em>However, that comes at the cost of full control over <strong>my</strong> website</em>. The miniscule time and cost investment is a small price to pay for <strong>actually owning your own things</strong> (Doesn't that sound ridiculous? Most people don't online).
</p>

<h3> Less is more </h3>

<p>My second goal was a design goal: KISS, or, <b>K</b>eep <b>I</b>t <b>S</b>imple, <b>S</b>my friend. I'm a minimalist. I don't like owning branded T-shirts or physical papers. I don't even own a mouse! It doesn't make sense for me to over-engineer a simple static text-based website. I only load what I need, and I don't abuse Javascript like Google (see: Google Docs protected mode. How does even exist??).
</p>

<p>If you're curious, <a href="https://github.com/simonxiang1/website" target="_blank">check out the source code</a>— of course, open source is essential for digital freedom. In terms of design, I took inspiration from <a href="http://motherfuckingwebsite.com target="_blank"">these</a> <a href="http://bettermotherfuckingwebsite.com" target="_blank">three</a> <a href="https://lukesmith.xyz" target="_blank">websites</a> (once again, I don't condone swearing). There's one more reason left as to why my website looks like this...

<h3> The final reason </h3>
<p>
I'm lazy and not very good at HTML/CSS.
</p>
]]></description>
</item>













<item>
<title>Topological Continuity: Simplicity in Abstraction</title>
<guid>https://simonxiang.xyz/blog/topological-continuity-simplicity-in-abstraction</guid>
<pubDate>Sat, 11 Jul 2020 14:05:15 -0500</pubDate>
<description><![CDATA[
<p>Abstraction is often looked down upon by engineers and physicists as something mathematicians do to make life harder for themselves, obfuscate simple things, and make math less applicable in real life.</p> 
<p>However, the opposite is actually true— restricting your viewpoint to one set or space (say, \( \mathbb{R}^n \)) makes things much more complicated than they have to be. <strong>Beauty and simplicity lie in levels of abstraction.</strong> Let's take a look at an example, some prerequisites include basic knowledge of metric spaces and topological spaces. </p>

<p>Here's the standard rigorous definition of continuity from Real Analysis. We say \( f: \mathbb{R} \to \mathbb{R} \) is <b>continuous</b> at \( x_0 \in \mathbb{R} \) if for all \( \epsilon > 0 \), there exists a \( \delta > 0 \) such that 

<p>
\[ 
    |x - x_0| < \delta \implies |f(x)-f(x_0)| < \epsilon 
\]
</p>

for all \( x \in \mathbb{R} \). Of course the domain doesn't have to be \( \mathbb{R} \), it can be any interval, say \( A \). When a function is continuous for all \( x_0 \in A \), it's <em>continuous on</em> \( A \). If a function is continuous on its domain, we just say it's <em>continuous</em>.
</p>

<p> That probably triggered some flashbacks for some poor Calculus students learning about limits for the first time (I too, feared the epsilon-delta dynamic duo up until I took Analysis). It doesn't have to be this scary! Forget the strange Greek letters, let's make this definition more powerful and simple at the same time.</p>

<p> Observe that the absolute value signs are just measuring the <em>distance</em> between two points in $\mathbb{R}$, indeed, this seems like a job more suited for <b>metric spaces</b>. Let $\mathbb{R}$ be equipped with the standard metric $d$ such that $d: \mathbb{R} \times \mathbb{R} \to \mathbb{R}, \, d(x,y) = |x-y|$. You can easily check that $d$ satisfies the conditions for a metric, so $(\mathbb{R},d)$ is a metric space. Now let's rewrite our previous definition in metric terms: \(f: \mathbb{R} \to \mathbb{R} \) is continuous at \( x_0 \in \mathbb{R} \) if for all \( \epsilon > 0 \), there exists a \( \delta > 0 \) such that 

\[
    d(x, x_0) < \delta \implies d(f(x),f(x_0)) < \epsilon.
\]

<p>Recall the definition of an <em>open ball</em> in a metric space. Let $(X,d)$ be a metric space, $x_0 \in X, \gamma \gt 0$. Then we define an <u>open ball</u> as 
\[ B(x_0, \gamma) = \{ x \in X \mid d(x,x_0) < \gamma \}. \]
Intuitively, it's the set of all points a certain distance <em>gamma</em> away from another point $x_0$. Notice that the two expressions in the definition just refer to points $(x \in \mathbb{R})$ a certain positive distance $(\epsilon, \delta)$ away from another point $(x_0)$, hmmm...
\[
    d(x, x_0) < \delta \implies d(f(x),f(x_0)) < \epsilon
\]
implies that 
\[
    x \in B(x_0, \delta) \implies f(x) \in B(f(x_0), \epsilon)
\]
which subsequently implies
\[
    f\left[B(x_0, \delta) \right] \subset B(f(x_0),\epsilon).
\]
The last implication may seem somewhat arcane, but just look closer and you'll see it quickly follows from the definition of the image of a set under a function. But wait! We haven't even seen the final form of continuity yet! Recall that every metric space generates a topology, and that the metric topology is generated by defining open balls as open sets... Now we're ready to define the final layer of abstraction, topological continuity. 
</p>

<p><b><u>Definition:</u></b> Let $(X, \tau_x )$ and $(Y, \tau_y )$ be <em>topological spaces</em>. Then a function $f: X \to Y$ is <b>continuous</b> if for all open sets $H \in \tau_y, \, f^{-1}(H) \in \tau_x.$
</p>

<p>The moment of truth: we transformed a convoluted epsilon delta definition into a simple, elegant, and much more widely applicable statement: <em>"A function is <b>continuous</b> if the pre-image of an open set is open."</em> Not only is this statement much simpler, it also applies to all sorts of spaces— the set of binary sequences, a really long line, co-finite sets, anything. Isn't that great?
</p>
]]></description>
</item>































</channel>
</rss>
